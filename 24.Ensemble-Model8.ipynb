{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 3593480383974045428, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 9214062756\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 3026034058643545384\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(777)\n",
    "\n",
    "from keras.models import Model,load_model\n",
    "from keras import Input\n",
    "from keras.layers import concatenate,Dense,Flatten,Dropout\n",
    "from keras.preprocessing.image import  ImageDataGenerator\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Version 2.2.4\n",
      "tensorflow Version 1.9.0\n",
      "dim_ordering: tf\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Keras Version\", keras.__version__)\n",
    "print(\"tensorflow Version\", tf.__version__)\n",
    "print(\"dim_ordering:\", K.image_dim_ordering())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_classes = 2\n",
    "batch_size = 8\n",
    "img_height, img_width = 224, 224\n",
    "input_shape = (img_height, img_width, 3)\n",
    "epochs = 50\n",
    "\n",
    "nb_train_samples = 59168\n",
    "nb_validation_samples = 18384\n",
    "nb_test_samples = 967"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/train/'\n",
    "validation_dir = 'data/validation'\n",
    "test_dir = 'data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 59168 images belonging to 2 classes.\n",
      "Found 18384 images belonging to 2 classes.\n",
      "Found 967 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "random_seed = np.random.seed(1142)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed = random_seed,\n",
    "    shuffle = True,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed = random_seed,\n",
    "    shuffle = True,\n",
    "    class_mode='categorical')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed = random_seed,\n",
    "    shuffle = False,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_opt = Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-5)\n",
    "sgd_opt = SGD(lr=1e-06, momentum=0.0, decay=0.0, nesterov=False)\n",
    "rmsp_opt = RMSprop(lr=1e-4, decay=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, layer in enumerate(NasNetLarge_model.layers):\n",
    "#     print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NasNetLarge_model = load_model('models/5.NASNetLarge-new-ISBI19-Model.h5', compile=False)\n",
    "NasNetLarge_model = Model(inputs= NasNetLarge_model.input,outputs = NasNetLarge_model.get_layer('dense_3').output, name='NasNetLarge_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InceptionResNetV2_model = load_model('models/5.InceptionResNetV2-ISBI19-Model.h5')\n",
    "# InceptionResNetV2_model = Model(inputs=resnet_model.input,outputs=resnet_model.get_layer('dense_2').output,name='InceptionResNetV2_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet201_model = load_model('models/12.DenseNet201-ISIB19-DataSet-Model.h5')\n",
    "# DenseNet201_model = Model(inputs=DenseNet201_model.input, outputs=DenseNet201_model.get_layer('dense_3').output, name='DenseNet201_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InceptionV3_model = load_model('models/9.InceptionV3-ISIB19-DataSet-Model.h5')\n",
    "# InceptionV3_model = Model(inputs = InceptionV3_model.input,outputs = InceptionV3_model.get_layer('dense_3').output, name='InceptionV3_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG19_model = load_model('models/11.VGG19-ISBI19-Model.h5')\n",
    "VGG19_model = Model(inputs = VGG19_model.input,outputs = VGG19_model.get_layer('dense_3').output, name='VGG19_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Input(shape=(224, 224,3),name='img')\n",
    "\n",
    "feature1=NasNetLarge_model(img)\n",
    "# feature2=InceptionV3_model(img)\n",
    "\n",
    "# for layer in InceptionV3_model.layers[:300]:  \n",
    "#     layer.trainable = False \n",
    "# for layer in InceptionV3_model.layers[300:]:  \n",
    "#     layer.trainable = True  \n",
    "\n",
    "feature3=VGG19_model(img)\n",
    "# for layer in VGG19_model.layers[:170]:  \n",
    "#     layer.trainable = False \n",
    "# for layer in VGG19_model.layers[170:]:  \n",
    "#     layer.trainable = True  \n",
    "\n",
    "# x = concatenate([feature1,feature2,feature3])\n",
    "x = concatenate([feature1,feature3])\n",
    "\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64,activation='relu')(x)\n",
    "x = Dropout(0.25)(x)\n",
    "output = Dense(2, activation='softmax', name='output')(x)\n",
    "\n",
    "model=Model(inputs=img,outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install graphviz\n",
    "# plot_model(model=model,to_file='ensemble.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7396/7396 [==============================] - 5113s 691ms/step - loss: 0.5566 - acc: 0.7804 - val_loss: 0.3993 - val_acc: 0.8784\n",
      "Epoch 2/50\n",
      "7396/7396 [==============================] - 5008s 677ms/step - loss: 0.4099 - acc: 0.8579 - val_loss: 0.3129 - val_acc: 0.9061\n",
      "Epoch 3/50\n",
      "7396/7396 [==============================] - 4986s 674ms/step - loss: 0.3583 - acc: 0.8658 - val_loss: 0.2616 - val_acc: 0.9352\n",
      "Epoch 4/50\n",
      "7396/7396 [==============================] - 5041s 682ms/step - loss: 0.3275 - acc: 0.8737 - val_loss: 0.2652 - val_acc: 0.9281\n",
      "Epoch 5/50\n",
      "7396/7396 [==============================] - 5014s 678ms/step - loss: 0.3137 - acc: 0.8779 - val_loss: 0.3160 - val_acc: 0.9115\n",
      "Epoch 6/50\n",
      "7396/7396 [==============================] - 5044s 682ms/step - loss: 0.2808 - acc: 0.8921 - val_loss: 0.1881 - val_acc: 0.9425\n",
      "Epoch 7/50\n",
      "7396/7396 [==============================] - 5045s 682ms/step - loss: 0.2612 - acc: 0.8966 - val_loss: 0.1783 - val_acc: 0.9489\n",
      "Epoch 8/50\n",
      "7396/7396 [==============================] - 5045s 682ms/step - loss: 0.2442 - acc: 0.9021 - val_loss: 0.1724 - val_acc: 0.9442\n",
      "Epoch 9/50\n",
      "7396/7396 [==============================] - 5035s 681ms/step - loss: 0.2335 - acc: 0.9053 - val_loss: 0.1729 - val_acc: 0.9426\n",
      "Epoch 10/50\n",
      "7396/7396 [==============================] - 5049s 683ms/step - loss: 0.2256 - acc: 0.9074 - val_loss: 0.1562 - val_acc: 0.9490\n",
      "Epoch 11/50\n",
      "7396/7396 [==============================] - 5051s 683ms/step - loss: 0.2213 - acc: 0.9072 - val_loss: 0.1686 - val_acc: 0.9448\n",
      "Epoch 12/50\n",
      "7396/7396 [==============================] - 4943s 668ms/step - loss: 0.2154 - acc: 0.9090 - val_loss: 0.1498 - val_acc: 0.9522\n",
      "Epoch 13/50\n",
      "7396/7396 [==============================] - 4901s 663ms/step - loss: 0.2113 - acc: 0.9108 - val_loss: 0.1453 - val_acc: 0.9556\n",
      "Epoch 14/50\n",
      "7396/7396 [==============================] - 4897s 662ms/step - loss: 0.2093 - acc: 0.9108 - val_loss: 0.1616 - val_acc: 0.9470\n",
      "Epoch 15/50\n",
      "7396/7396 [==============================] - 4894s 662ms/step - loss: 0.2060 - acc: 0.9120 - val_loss: 0.1528 - val_acc: 0.9521\n",
      "Epoch 16/50\n",
      "3045/7396 [===========>..................] - ETA: 45:05 - loss: 0.2156 - acc: 0.9063"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=adam_opt, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size,\n",
    "    workers=8)\n",
    "\n",
    "# with open('ensemble2_history.txt','w') as f:\n",
    "#     f.write(str(history.history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/24.Ensemble8-ISBI19-Model.h5')\n",
    "model.save_weights('models/24.Ensemble8-ISBI19-Weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Acc', 'Test Acc'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Loss', 'Test Loss'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "N = epochs\n",
    "plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate_generator(validation_generator, steps=50)\n",
    "\n",
    "print ('Validation Score: ', score[0])\n",
    "print ('Validation Accuracy: ',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = test_generator.filenames\n",
    "truth = test_generator.classes\n",
    "label = test_generator.class_indices\n",
    "indexlabel = dict((value, key) for key, value in label.items())\n",
    "\n",
    "predicts = model.predict_generator(test_generator, steps=test_generator.samples/test_generator.batch_size, verbose=1)\n",
    "predict_class = np.argmax(predicts, axis=1)\n",
    "errors = np.where(predict_class != truth)[0]\n",
    "print(\"No of errors = {}/{}\".format(len(errors),test_generator.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(truth,predict_class)\n",
    "\n",
    "labels = []\n",
    "for k,v in indexlabel.items():\n",
    "    labels.append(v)\n",
    "    \n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion Matrix')\n",
    "\n",
    "    print(cm)\n",
    "#     fig = plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "#     plt.savefig('plots/17.Ensemble-Models1-CM.png', bbox_inches='tight', dpi = 100) \n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "plot_confusion_matrix(cm, classes=labels,title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "y_pred = predicts\n",
    "y_pred_probabilities=y_pred\n",
    "\n",
    "y_pred = np.argmax(y_pred,axis = 1) \n",
    "y_actual = test_generator.classes\n",
    "\n",
    "classnames=[]\n",
    "for classname in test_generator.class_indices:\n",
    "    classnames.append(classname)\n",
    "\n",
    "confusion_mtx = confusion_matrix(y_actual, y_pred) \n",
    "print(confusion_mtx)\n",
    "target_names = classnames\n",
    "print(classification_report(y_actual, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=sum(sum(cm))\n",
    "\n",
    "accuracy = (cm[0,0]+cm[1,1]) / total\n",
    "print ('Accuracy : ', accuracy*100)\n",
    "\n",
    "sensitivity = cm[0,0]/(cm[0,0]+cm[1,0])\n",
    "print('Sensitivity : ', sensitivity*100 )\n",
    "\n",
    "Specificity = cm[1,1]/(cm[1,1]+cm[0,1])\n",
    "print('Specificity : ', Specificity*100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "th = 0.3\n",
    "\n",
    "acc = accuracy_score(truth,predict_class > th)\n",
    "prec = precision_score(truth,predict_class > th)\n",
    "f1 = f1_score(truth,predict_class > th)\n",
    "recall = recall_score(truth,predict_class > th)\n",
    "\n",
    "print('Accuracy:  {:.4f}'.format(acc*100))\n",
    "print('Precision: {:.4f}'.format(prec*100))\n",
    "print('Recall:    {:.4f}'.format(recall*100))\n",
    "print('F1:        {:.4f}'.format(f1*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_generator.classes, predict_class)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "lw = 1\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "#plotting sensitivity and specificity\n",
    "plt.figure()\n",
    "plt.plot(thresholds, 1-fpr, label = 'specificity')\n",
    "plt.plot(thresholds, tpr, label = 'sensitivity')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Threshold value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
